# -*- coding: utf-8 -*-
"""Data_Analysis_LeadScoring.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11NdZPYARypF_7xkqAUs2E1BtrTSH3TOI
"""

# Install openpyxl to read Excel files
!pip install openpyxl

#import libraries for analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Read the CSV file
df = pd.read_csv('/content/Lead Scoring.csv')

# Display shape and first few rows
print("Shape:", df.shape)
df.head()

print("Shape:", df.shape)
df.head()

"""**Basic EDA (Exploratory Data Analysis)**"""

#A. Check for missing values
df.isnull().sum()

"""I will give **highest importance** to 2 features: **Lead Source**(because â€” tells where the best leads come from)  & **Total Time Spent on Website**(because-
indicates buyer interest)
"""

df.isnull().sum().sort_values(ascending=False).head(15)

"""Total size of the data >>>Shape: (9240, 37)
Missing in:
 Lead Source > 36
 Total Time Spent on Website> 0

 Percentage of missing value to total value in Lead Source: 0.389%

As Lead Source feature has low missing value and high business relevance, we will KEEP & IMPUTE IT
"""

#impute the Lead Source- values
df['Lead Source'] = df['Lead Source'].fillna(df['Lead Source'].mode()[0])

#B. Check data types & basic info
df.info()

import seaborn as sns
sns.heatmap(df.isnull(), cbar=False)
#This shows you where the data is missing in a colorful way.

df.columns = df.columns.str.strip()

df.columns.tolist()

df.drop(columns=[
    'Asymmetrique Profile Index',
    'Tags'
], inplace=True)

cols_to_drop = [
    'Asymmetrique Activity Index',
    'Asymmetrique Profile Index',
    'Asymmetrique Activity Score',
    'Asymmetrique Profile Score',
    'Lead Quality',
    'Tags',
    'Lead Profile',
    'What matters most to you in choosing a course'
]

df.drop(columns=[col for col in cols_to_drop if col in df.columns], inplace=True)

#Impute value
for col in df.columns:
    print(f"'{col}'")

#Strip All Column Names
df.columns = df.columns.str.strip()

#impute
df['Specialization'] = df['Specialization'].fillna('Unknown')
df['What is your current occupation'] = df['What is your current occupation'].fillna('Unknown')
df['City'] = df['City'].fillna('Unknown')
df['Country'] = df['Country'].fillna('India')  # Based on assumption

#recheck missing data
df.isnull().sum()
df.isnull().sum().sort_values(ascending=False).head(15)

"""**CLEANING PLAN**: 1. Low missing values (we fill)

2. Medium-high missing values (we decide case-by-case)

3. Very high missing values (we drop)

"""

# Fill low-missing columns
#both for numeric and text columns
df['Lead Source'] = df['Lead Source'].fillna(df['Lead Source'].mode()[0])
df['TotalVisits'] = df['TotalVisits'].fillna(df['TotalVisits'].median())
df['Page Views Per Visit'] = df['Page Views Per Visit'].fillna(df['Page Views Per Visit'].median())
df['Last Activity'] = df['Last Activity'].fillna(df['Last Activity'].mode()[0])
df['Specialization'] = df['Specialization'].fillna(df['Specialization'].mode()[0])
df['City'] = df['City'].fillna(df['City'].mode()[0])
df['Country'] = df['Country'].fillna(df['Country'].mode()[0])

#C. Target variable distribution
df['Converted'].value_counts(normalize=True) * 100
sns.countplot(x='Converted', data=df)
plt.title('Conversion Distribution')

"""** Univariate & Bivariate Analysis**"""

#A. Numerical Features
df.describe()

# Plot histograms
df[['TotalVisits', 'Total Time Spent on Website', 'Page Views Per Visit']].hist(bins=20, figsize=(10,5))

#B. Categorical Features vs Conversion
# Example: Lead Source vs Conversion
lead_source_conv = pd.crosstab(df['Lead Source'], df['Converted'], normalize='index') * 100
lead_source_conv.plot(kind='bar', stacked=True)
plt.title('Conversion % by Lead Source')
plt.ylabel('% Conversion')

"""Here, **1** means **the lead converted**. The sales/marketing team successfully converted the lead into a customer.

Here, **0** means the **lead did not convert.** The lead either dropped off or was not interested after follow-up.

**Conversion by Lead Source**
"""

plt.figure(figsize=(12,6))
sns.barplot(x='Lead Source', y='Converted', data=df)
plt.xticks(rotation=45)
plt.title('Conversion Rate by Lead Source')
plt.show()

# Group data by TotalVisits and get mean conversion rate at each level
visit_conversion = df.groupby('TotalVisits')['Converted'].mean().reset_index()

# Plot line chart
plt.figure(figsize=(12,6))
plt.plot(visit_conversion['TotalVisits'], visit_conversion['Converted'], marker='o')
plt.title('Lead Conversion Rate by Total Website Visits')
plt.xlabel('Total Website Visits')
plt.ylabel('Conversion Rate')
plt.grid(True)
plt.show()

"""**Handle Missing Values & Feature Engineering**

Here, we will drop columns with too many nulls

Fill NA values where meaningful (e.g., â€œSelectâ€ â†’ np.nan)

Create new features like:

Engagement_Level (based on time spent + visits)

Communication_Pref (combining Do Not Call + Do Not Email)
"""

# Replace 'Select' with NaN
df.replace('Select', np.nan, inplace=True)

# Feature: Engagement Score
df['Engagement Score'] = df['TotalVisits'] * df['Page Views Per Visit']

# Drop columns with >50% missing values
missing_percent = df.isnull().mean()
df = df.drop(columns=missing_percent[missing_percent > 0.5].index)

"""** Correlation & Insights**"""

# Correlation matrix
# Filter only numeric columns
numeric_df = df.select_dtypes(include=[np.number])

# Plot correlation matrix
plt.figure(figsize=(10,6))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""**Data Preprocessing (Final Cleanup before Modeling)**"""

print(df.columns.tolist())

df.columns = df.columns.str.strip()

binary_cols = [
    'Do Not Email',
    'Receive More Updates About Our Courses',
    'Update me on Supply Chain Content',
    'Get updates on DM Content',
    'I agree to pay the amount through cheque',
    'A free copy of Mastering The Interview',
    'Through Recommendations'
]

for col in binary_cols:
    if col in df.columns:
        df[col] = df[col].map({'Yes': 1, 'No': 0})
    else:
        print(f"Column not found: {col}")

#reload file
df = pd.read_csv('/content/Lead Scoring.csv')
df.head()

#use the file to do BINARY MAPPING
#Binary Mapping Code
binary_cols = [
    'Do Not Email',
    'Do Not Call',
    'Get updates on DM Content',
    'I agree to pay the amount through cheque',
    'A free copy of Mastering The Interview'
]

for col in binary_cols:
    if col in df.columns:
        df[col] = df[col].map({'Yes': 1, 'No': 0})
    else:
        print(f"Column not found: {col}")

# Example: One-hot encoding categorical columns
df = pd.get_dummies(df, drop_first=True)

# Drop ID columns only if they exist
cols_to_drop = ['Prospect ID', 'Lead Number']
df.drop(columns=[col for col in cols_to_drop if col in df.columns], inplace=True)

#Seperate features from target
X = df.drop('Converted', axis=1)
y = df['Converted']

df.head()

#Train test split
from sklearn.model_selection import train_test_split

# 70% train, 30% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""Model Building: Will build ML Models to predict lead conversion.
Using:

1. Logistic Regression
2. Decision Tree
3. Random Forest
4. XGBoost
"""

#Logistics Regression
X_train.isnull().sum().sort_values(ascending=False).head(10)

df.head()

from sklearn.impute import SimpleImputer

# Use median to fill missing values
imputer = SimpleImputer(strategy='median')
X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

#A)Logistics Regression
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train Logistic Regression model
lr = LogisticRegression(max_iter=2000)
lr.fit(X_train_scaled, y_train)

from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score

y_pred = lr.predict(X_test_scaled)
y_prob = lr.predict_proba(X_test_scaled)[:, 1]

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print("AUC Score:", roc_auc_score(y_test, y_prob))

#B)Decison Tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# 1. Train the model
dt = DecisionTreeClassifier(random_state=42, max_depth=5)  # max_depth to avoid overfitting
dt.fit(X_train, y_train)

# 2. Predict
y_pred_dt = dt.predict(X_test)
y_prob_dt = dt.predict_proba(X_test)[:, 1]

# 3. Evaluate
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_dt))
print("\nClassification Report:\n", classification_report(y_test, y_pred_dt))
print("\nAUC Score:", roc_auc_score(y_test, y_prob_dt))

#Visulaize Decision tree
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 10))
plot_tree(dt, filled=True, feature_names=X_train.columns, class_names=['Not Converted', 'Converted'], rounded=True)
plt.title("Decision Tree Visualization")
plt.show()

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

y_pred_rf = rf.predict(X_test)
y_prob_rf = rf.predict_proba(X_test)[:, 1]

"""**Model Evaluation**"""

#Model Evaluation
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve

# Logistic Regression Evaluation
print("Logistic Regression")
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print("AUC Score:", roc_auc_score(y_test, y_prob))

# Random Forest Evaluation
print("\nRandom Forest")
print(confusion_matrix(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))
print("AUC Score:", roc_auc_score(y_test, y_prob_rf))

# ROC Curve
import matplotlib.pyplot as plt

fpr, tpr, _ = roc_curve(y_test, y_prob_rf)
plt.plot(fpr, tpr, label='Random Forest')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

""" **XGBoost** Code Example

 XGBoost stands for Extreme Gradient Boosting. It is an optimized implementation of Gradient Boosting Machines (GBMs), built for speed and performance.

It builds models sequentially, where each new tree tries to correct the errors made by previous ones.

Instead of learning from scratch, each new tree focuses on improving whatâ€™s already been learned â€” this is boosting.

It uses gradient descent to minimize loss, which is why itâ€™s called Gradient Boosting.
"""

pip install xgboost

import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# 1. Train the XGBoost model
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb_model.fit(X_train, y_train)

# 2. Predict
y_pred_xgb = xgb_model.predict(X_test)
y_prob_xgb = xgb_model.predict_proba(X_test)[:, 1]

# 3. Evaluate
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))
print("\nClassification Report:\n", classification_report(y_test, y_pred_xgb))
print("\nAUC Score:", roc_auc_score(y_test, y_prob_xgb))

import matplotlib.pyplot as plt

xgb.plot_importance(xgb_model, max_num_features=10, importance_type='gain')
plt.title("Top 10 Important Features")
plt.show()

# Logistic Regression Predictions
y_pred = lr.predict(X_test_scaled)
y_prob = lr.predict_proba(X_test_scaled)[:, 1]

# Now run evaluations
print("ðŸ” Logistic Regression")
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print("AUC Score:", roc_auc_score(y_test, y_prob))

#Full Model Evaluation
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Evaluate Logistic Regression
print("ðŸ” Logistic Regression")
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print("AUC Score:", roc_auc_score(y_test, y_prob))

# Evaluate Random Forest
print("\nðŸŒ² Random Forest")
print(confusion_matrix(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))
print("AUC Score:", roc_auc_score(y_test, y_prob_rf))

# Evaluate Decision Tree
print("\nðŸŒ³ Decision Tree")
print(confusion_matrix(y_test, y_pred_dt))
print(classification_report(y_test, y_pred_dt))
print("AUC Score:", roc_auc_score(y_test, y_prob_dt))

# Evaluate XGBoost
print("\nâš¡ XGBoost")
print(confusion_matrix(y_test, y_pred_xgb))
print(classification_report(y_test, y_pred_xgb))
print("AUC Score:", roc_auc_score(y_test, y_prob_xgb))

# Plot ROC Curve for all models
fpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob)
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)
fpr_dt, tpr_dt, _ = roc_curve(y_test, y_prob_dt)
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_prob_xgb)

plt.figure(figsize=(10, 6))
plt.plot(fpr_lr, tpr_lr, label='Logistic Regression')
plt.plot(fpr_rf, tpr_rf, label='Random Forest')
plt.plot(fpr_dt, tpr_dt, label='Decision Tree')
plt.plot(fpr_xgb, tpr_xgb, label='XGBoost')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal reference line

plt.title('ROC Curve Comparison')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.grid(True)
plt.show()

"""**Lead Scoring**"""

#  Split into features and target
X = df.drop('Converted', axis=1)  # Features
y = df['Converted']               # Target

"""Handle categorical features: Convert categorical columns to numeric"""

X = pd.get_dummies(X, drop_first=True)

#Train-test split
 from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

#Feature Scaling
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Drop non-numeric or ID columns BEFORE splitting
X = df.drop(columns=['Converted', 'Prospect ID', 'Lead Number'], errors='ignore')
y = df['Converted']

# Split the data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Preprocessing pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features)
], remainder='drop')  # drop non-numeric for now

clf = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=1000))
])

# Train model
clf.fit(X_train, y_train)

# Predict lead scores
y_pred_proba = clf.predict_proba(X_test)[:, 1] * 100

"""Lead scoring"""

# Create a copy to avoid modifying X_test directly
lead_results = X_test.copy()

# Add lead scores (predicted probabilities * 100)
lead_results['Lead Score'] = y_pred_proba

def classify_lead(score):
    if score >= 70:
        return 'Hot Lead'
    elif score >= 40:
        return 'Warm Lead'
    else:
        return 'Cold Lead'

lead_results['Lead Category'] = lead_results['Lead Score'].apply(classify_lead)

# Show top 10 leads by score
lead_results.sort_values(by='Lead Score', ascending=False).head(10)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6,4))
sns.countplot(data=lead_results, x='Lead Category', order=['Hot Lead', 'Warm Lead', 'Cold Lead'], palette='coolwarm')
plt.title("Lead Category Distribution")
plt.xlabel("Lead Category")
plt.ylabel("Number of Leads")
plt.show()

#download modified_leadscoring dataset file
lead_results.to_csv('modified_leadscoring.csv', index=False)

# Re-run model and get predictions first if not already done
# Assuming clf is your trained model pipeline

y_pred_proba = clf.predict_proba(X_test)[:, 1] * 100

# Create a copy of X_test for lead scoring
lead_results = X_test.copy()
lead_results['Lead Score'] = y_pred_proba

# Classify into categories
def classify_lead(score):
    if score >= 70:
        return 'Hot Lead'
    elif score >= 40:
        return 'Warm Lead'
    else:
        return 'Cold Lead'

lead_results['Lead Category'] = lead_results['Lead Score'].apply(classify_lead)

lead_results.to_csv('modified_leadscoring.csv', index=False)

# Download the file
from google.colab import files
files.download('modified_leadscoring.csv')

"""The document does not make sense- let me recheck it"""

# Your cleaned result should look like this:
# Columns: Lead Source, Total Visits, Lead Score, Lead Category, etc.
# Rows: One per lead

print(lead_results.shape)
lead_results.head()

#recreate lead result
# Assuming your original X_test is still available
lead_results = X_test.copy()

# Add the predicted probabilities
lead_results['Lead Score'] = y_pred_proba

# Optional: Add classification
def classify_lead(score):
    if score >= 70:
        return 'Hot Lead'
    elif score >= 40:
        return 'Warm Lead'
    else:
        return 'Cold Lead'

lead_results['Lead Category'] = lead_results['Lead Score'].apply(classify_lead)

#Reload Your Clean Data Before One-Hot Encoding
df = pd.read_csv("/content/Lead Scoring.csv")  # wherever your cleaned, non-transformed data is

df.drop(columns=['Prospect ID', 'Lead Number'], errors='ignore', inplace=True)

binary_cols = [
    'Do Not Email', 'Do Not Call',
    'Receive More Updates About Our Courses',
    'Update me on Supply Chain Content',
    'Get updates on DM Content',
    'I agree to pay the amount through cheque',
    'A free copy of Mastering The Interview',
    'Through Recommendations'
]

for col in binary_cols:
    if col in df.columns:
        df[col] = df[col].map({'Yes': 1, 'No': 0})

from sklearn.preprocessing import OneHotEncoder

categorical_cols = df.select_dtypes(include='object').columns.tolist()
categorical_cols = [col for col in categorical_cols if col not in binary_cols]

df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

lead_results = X_test.copy()
lead_results['Lead Score'] = y_pred_proba

def classify_lead(score):
    if score >= 70:
        return 'Hot Lead'
    elif score >= 40:
        return 'Warm Lead'
    else:
        return 'Cold Lead'

lead_results['Lead Category'] = lead_results['Lead Score'].apply(classify_lead)

df.head()

columns_to_keep = [
    'Prospect ID', 'Lead Source', 'Specialization', 'City',
    'TotalVisits', 'Total Time Spent on Website', 'Page Views Per Visit',
    'Converted'
]

df = df[columns_to_keep]

df['Lead Source'].fillna('Unknown', inplace=True)
df['Specialization'].fillna('Unknown', inplace=True)
df['City'].fillna('Unknown', inplace=True)

# Drop rows missing engagement (you can impute instead if you want)
df.dropna(subset=['TotalVisits', 'Total Time Spent on Website', 'Page Views Per Visit'], inplace=True)

df['Engagement Score'] = df['TotalVisits'] * df['Page Views Per Visit']

# Simulated score: more time spent, more engagement = better
df['Lead Score'] = (
    0.3 * df['TotalVisits'] +
    0.3 * df['Page Views Per Visit'] +
    0.4 * df['Total Time Spent on Website']
)

# Scale between 0 and 100
df['Lead Score'] = (df['Lead Score'] / df['Lead Score'].max()) * 100

def classify_lead(score):
    if score >= 70:
        return 'Hot Lead'
    elif score >= 40:
        return 'Warm Lead'
    else:
        return 'Cold Lead'

df['Lead Category'] = df['Lead Score'].apply(classify_lead)

#chart
import matplotlib.pyplot as plt

# Count the number of leads in each category
lead_counts = df['Lead Category'].value_counts()

# Define custom colors for each category
color_map = {
    'Hot Lead': '#E74C3C',    # red
    'Warm Lead': '#F4D03F',   # yellow
    'Cold Lead': '#5DADE2'    # blue
}

# Extract colors in the right order
colors = [color_map[label] for label in lead_counts.index]

# Create the pie chart
plt.figure(figsize=(6,6))
plt.pie(
    lead_counts,
    labels=lead_counts.index,
    autopct='%1.1f%%',
    startangle=140,
    colors=colors
)

plt.title("Lead Category Proportions", fontsize=14)
plt.axis('equal')  # Makes it a perfect circle
plt.show()

df.head()

df.head()
df.shape

df.head()

# Replace 'Select' in 'Specialization' with 'Business'
df['Specialization'] = df['Specialization'].replace('Select', 'Business')

# Replace 'Select' in 'City' with 'India'
df['City'] = df['City'].replace('Select', 'India')

df.head()

#dowload the modified excel file
df.to_csv("lead_scoring_output_GoogleColab.csv", index=False)
df.head()
df.shape